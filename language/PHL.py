import re
import unicodedata
from typing import Dict, List


# ===============================
# è²å¾‹å®¾è¯­æ•°å­— â†’ é˜¿æ‹‰ä¼¯æ•°å­— æ˜ å°„
# ï¼ˆå”¯ä¸€æ–°å¢çš„æ˜ å°„è§„åˆ™ï¼‰
# ===============================
FIL_NUMBER_MAP = {
    "ZERO": "0",
    "ISA": "1",
    "DALAWA": "2",
    "TATLO": "3",
    "APAT": "4",
    "LIMA": "5",
    "ANIM": "6",
    "PITO": "7",
    "WALO": "8",
    "SIYAM": "9",
}


def normalize(text: str) -> str:
    """
    è²å¾‹å®¾è¯­å®Œæ•´è§„èŒƒåŒ–è„šæœ¬ï¼ˆä¿®æ­£ç‰ˆï¼‰

    ä¸»è¦åŠŸèƒ½ï¼š
    1. é‡å¤å­—æ¯å®Œå…¨å½’çº¦ï¼ˆsobrang â†’ sobrangï¼‰
    2. æ™ºèƒ½ç¼©å†™å±•å¼€ï¼ˆ'yung â†’ ang, d'yan â†’ diyanï¼‰
    3. å¤–æ¥è¯æ ‡å‡†åŒ–ï¼ˆcafÃ© â†’ cafe, azÃºcar â†’ asukalï¼‰
    4. æ ‡ç‚¹/ç©ºæ ¼æ¸…ç†
    5. URL/ç”µå­é‚®ä»¶ä¿æŠ¤
    """
    if not text.strip():
        return text

    # é¢„å¤„ç†ï¼šä¿æŠ¤URL/ç”µå­é‚®ä»¶
    protected_spans = []
    text = protect_special_content(text, protected_spans)

    # å¤„ç†é¡ºåºï¼ˆé‡è¦ï¼ï¼‰
    text = remove_accents(text)                  # å…ˆå»é‡éŸ³
    text = reduce_repeated_characters(text)      # å†å»é‡å­—æ¯
    text = expand_contractions(text)             # ç„¶åå±•å¼€ç¼©å†™
    text = standardize_spelling(text)            # æœ€åæ ‡å‡†åŒ–æ‹¼å†™

    # ===============================
    # è²å¾‹å®¾è¯­æ•°å­—è¯ â†’ é˜¿æ‹‰ä¼¯æ•°å­—
    # ï¼ˆä»…æ˜ å°„è§„åˆ™ï¼‰
    # ===============================
    for k, v in FIL_NUMBER_MAP.items():
        text = re.sub(rf"\b{k}\b", v, text, flags=re.IGNORECASE)

    # åå¤„ç†
    text = restore_protected_content(text, protected_spans)
    text = clean_text(text)

    text = text.upper()

    return text


def protect_special_content(text: str, protected_spans: List) -> str:
    """ä¿æŠ¤URLå’Œç”µå­é‚®ä»¶"""
    for pattern in [r'https?://\S+', r'\b[\w.-]+@[\w.-]+\.\w+\b']:
        for match in re.finditer(pattern, text):
            protected_spans.append({
                'start': match.start(),
                'end': match.end(),
                'original': match.group()
            })
            text = (
                text[:match.start()]
                + f" ğ™‹ğ™ğ™Šğ™ğ™€ğ˜¾ğ™ğ™€ğ˜¿_{len(protected_spans)-1} "
                + text[match.end():]
            )
    return text


def restore_protected_content(text: str, protected_spans: List) -> str:
    """æ¢å¤è¢«ä¿æŠ¤çš„å†…å®¹"""
    for i, span in enumerate(protected_spans):
        text = text.replace(f"ğ™‹ğ™ğ™Šğ™ğ™€ğ˜¾ğ™ğ™€ğ˜¿_{i}", span['original'])
    return text


def remove_accents(text: str) -> str:
    """ç§»é™¤é‡éŸ³ç¬¦å·ï¼ˆÃ© â†’ eï¼‰"""
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')


def reduce_repeated_characters(text: str) -> str:
    """é‡å¤å­—æ¯å®Œå…¨å½’çº¦"""
    return re.sub(r'([a-zA-Z])\1+', r'\1', text)


def expand_contractions(text: str) -> str:
    """è²å¾‹å®¾è¯­ç¼©å†™å±•å¼€ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰"""
    CONTRACTIONS = {
        # æ•´ä½“æ›¿æ¢ä¼˜å…ˆ
        r"'yung\b": "ang",
        r"'yng\b": "ang",
        r"'ung\b": "ang",
        r"d'yan\b": "diyan",
        r"n'ung\b": "ng",

        # é€šç”¨è§„åˆ™
        r"'y\b": "ang",
        r"'t\b": "at",
        r"'n\b": "ng",
        r"'di\b": "hindi",
        r"'pag\b": "kapag",
    }

    for pattern, replacement in CONTRACTIONS.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

    return text


def standardize_spelling(text: str) -> str:
    """æ ‡å‡†åŒ–æ‹¼å†™ï¼ˆå«å¤§å°å†™æ„ŸçŸ¥ï¼‰"""
    SPELLING_VARIANTS = {
        'azucar': 'asukal',
        'kompyuter': 'komputer',
        'pamilya': 'pamilya',
        'nang': 'ng',
        'nang ': 'ng ',
    }

    words = text.split()
    normalized_words = []

    for word in words:
        original_word = word
        word_lower = remove_accents(word.lower())

        # æ£€æŸ¥å˜ä½“ï¼ˆå¿½ç•¥æ ‡ç‚¹ï¼‰
        word_stem = re.sub(r'[^\w]', '', word_lower)
        if word_stem in SPELLING_VARIANTS:
            normalized_word = adjust_case(word, SPELLING_VARIANTS[word_stem])
            punctuation = re.sub(r'[\w]', '', original_word)
            normalized_words.append(normalized_word + punctuation)
        else:
            normalized_words.append(word)

    return ' '.join(normalized_words)


def adjust_case(original: str, replacement: str) -> str:
    """æ™ºèƒ½å¤§å°å†™è½¬æ¢"""
    if original.isupper():
        return replacement.upper()
    elif original.istitle():
        return replacement.capitalize()
    return replacement.lower()


def clean_text(text: str) -> str:
    """æœ€ç»ˆæ¸…ç†"""
    text = re.sub(r'([.,!?])\1+', r'\1', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def get_normalizer(language_code: str):
    if language_code.upper() in ('FIL', 'PHL'):
        return normalize
    raise ValueError(f"Unsupported language: {language_code}")
